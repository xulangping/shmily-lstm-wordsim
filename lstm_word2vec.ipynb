{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "The labeled training set dimension is:\n",
      "\n",
      "(25000, 3)\n",
      "The unlabeled training set dimension is:\n",
      "\n",
      "(25000, 3)\n",
      "(50000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from lib import data_split, features_word2vec, model_lstm, model_randomforest\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read data\n",
    "# Use the kaggle Bag of words vs Bag of popcorn data:\n",
    "# The data is downloaded from: \n",
    "# https://www.kaggle.com/c/word2vec-nlp-tutorial/data\n",
    "data = pd.read_csv(\"./data/labeledTrainData.tsv\", header=0,\n",
    "                   delimiter=\"\\t\", quoting=3, encoding=\"utf-8\")\n",
    "\n",
    "print(\"The labeled training set dimension is:\\n\")\n",
    "print(data.shape)\n",
    "\n",
    "data2 = pd.read_csv(\"./data/unlabeledTrainData.tsv\", header=0,\n",
    "                          delimiter=\"\\t\", quoting=3, encoding=\"utf-8\")\n",
    "\n",
    "print(\"The unlabeled training set dimension is:\\n\")\n",
    "print(data.shape)\n",
    "\n",
    "# Labeled data(data) and Unlabeled data(data2) \n",
    "# are combined to train the word2vec model\n",
    "data2.append(data)\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defined path of word2vec model \n",
    "model_path = \"./model/300features_40minwords_10context\"\n",
    "\n",
    "# If we have a pre-trained model we'd like to use, it can be loaded here directly. \n",
    "# Otherwise we will use the existing data to train it from scratch \n",
    "if not os.path.isfile(model_path):\n",
    "    model = features_word2vec.get_word2vec_model(data2, \"review\", num_features=300, downsampling=1e-3, model_name=model_path)\n",
    "else:\n",
    "    # After model is created, we can load it as an existing file\n",
    "    model = features_word2vec.load_word2vec_model(model_name=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34121, 300)\n"
     ]
    }
   ],
   "source": [
    "# Create word embeddings, which is essentially a dictionary\n",
    "# that maps word indices to word2vec features\n",
    "embedding_weights = features_word2vec.create_embedding_weights(model)\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34121, 300)\n"
     ]
    }
   ],
   "source": [
    "# We also need to prepare the word2vec features, so that they are \n",
    "# each word is now mapped to an index, consistents with the training embedding \n",
    "# Currently, we are limiting each review article to 500 words. \n",
    "# By default, we pad the LHS of each vector with zeros.  \n",
    "# e.g [ 0, 0, 0 .... 0.27, 0.89, 0.35]\n",
    "features = features_word2vec.get_indices_word2vec(data, \"review\", model, maxLength=500,\n",
    "                         writeIndexFileName=\"./model/imdb_indices.pickle\", padLeft=True )\n",
    "\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we separate data for training and validation \n",
    "y = data[\"sentiment\"]\n",
    "X_train, y_train, X_test, y_test = data_split.train_test_split_shuffle(y, features, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we are going to run a few experiments: \n",
    "# 1. classify IMDB data with LSTM + word2vec embedding only \n",
    "# 2. classify IMDB data with LSTM + one layer of CNN + word2vec embedding\n",
    "# 3. classify IMDB data with LSTM, no embedding \n",
    "# 4. Look back at the baseline, IMDB data + random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/3\n",
      "22500/22500 [==============================] - 6949s - loss: 0.5508 - acc: 0.7176 - val_loss: 0.4941 - val_acc: 0.7808\n",
      "Epoch 2/3\n",
      "22500/22500 [==============================] - 1714s - loss: 0.3811 - acc: 0.8396 - val_loss: 0.4109 - val_acc: 0.8324\n",
      "Epoch 3/3\n",
      "22500/22500 [==============================] - 1697s - loss: 0.2795 - acc: 0.8888 - val_loss: 0.3365 - val_acc: 0.8692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lib/model_lstm.py:29: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if embedding_weights == None and word2vec_model != None:\n",
      "lib/model_lstm.py:38: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  elif embedding_weights == None and word2vec_model == None:\n"
     ]
    }
   ],
   "source": [
    "# 1. classify IMDB data with LSTM + word2vec embedding only \n",
    "# Accuracy is 0.8692 after three iterations \n",
    "model_lstm.classif_imdb( X_train, y_train, X_test, y_test, embedding_weights = embedding_weights, \n",
    "                        dense_dim = 256, nb_epoch = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/3\n",
      "22500/22500 [==============================] - 746s - loss: 0.4262 - acc: 0.7949 - val_loss: 0.2962 - val_acc: 0.8788\n",
      "Epoch 2/3\n",
      "22500/22500 [==============================] - 745s - loss: 0.2274 - acc: 0.9107 - val_loss: 0.2696 - val_acc: 0.8904\n",
      "Epoch 3/3\n",
      "22500/22500 [==============================] - 745s - loss: 0.1566 - acc: 0.9432 - val_loss: 0.3256 - val_acc: 0.8764\n"
     ]
    }
   ],
   "source": [
    "# 2. classify IMDB data with LSTM + one layer of CNN + word2vec embedding\n",
    "# Accuracy is 0.8904 after two iterations. \n",
    "# Compare this with the keras LSTM + CNN code, without pre-embedding: \n",
    "# https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py \n",
    "# The accuracy there was 0.8498. \n",
    "model_lstm.classif_imdb(X_train, y_train, X_test, y_test, embedding_weights=embedding_weights, dense_dim=256,\n",
    "                        nb_epoch=3, include_cnn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/3\n",
      "22500/22500 [==============================] - 1803s - loss: 0.6063 - acc: 0.6760 - val_loss: 0.5221 - val_acc: 0.7588\n",
      "Epoch 2/3\n",
      "22500/22500 [==============================] - 1830s - loss: 0.4749 - acc: 0.7824 - val_loss: 0.6330 - val_acc: 0.6280\n",
      "Epoch 3/3\n",
      "22500/22500 [==============================] - 2055s - loss: 0.4266 - acc: 0.8073 - val_loss: 0.5160 - val_acc: 0.7344\n"
     ]
    }
   ],
   "source": [
    "# 3. classify IMDB data with LSTM, no embedding \n",
    "model_lstm.classif_imdb( X_train, y_train, X_test, y_test, embedding_weights = None, dense_dim = 256, nb_epoch = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8148\n",
      "[[ 992  258]\n",
      " [ 205 1045]]\n"
     ]
    }
   ],
   "source": [
    "# 4. Look back at the baseline, IMDB data + random forest\n",
    "# Compare with RF\n",
    "# Accuracy here is 0.815 \n",
    "features_avg_word2vec = features_word2vec.get_avgfeatures_word2vec(data, \"review\", model)\n",
    "X_train, y_train, X_test, y_test = data_split.train_test_split_shuffle(y, features_avg_word2vec, test_size=0.1)\n",
    "model_randomforest.classif(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
